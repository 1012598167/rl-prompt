experiment_type: translation
translation:
    task_name: "???"
    training_mode: "???"
    architecture: "transformer_small"
    mix_strategy: "mix"
    save_dir: "???"
    random_seed: 1
    # Training time
    num_epochs: 30
    save_frequency: 1
    num_batches_per_epoch: null
    # Paths
    checkpoint_path: null
    base_checkpoint_path: null
    # Other configs
    beam_width: 10
    top_k: null
    top_p: null
    learning_rate: 0.001
    gradient_clipping: true
    # SQL loss
    sql_loss_impl: "v2_v2r_v3_v3r"
    sql_loss_coefficients: null
    sql_loss_margin_constant: null
    sql_loss_margin_coefficient: null
    # Target model
    use_target_network: true
    target_sync_steps: null
    target_sync_method: "polyak"
    target_sql_loss_impl: null
    target_learning_rate: 0.001
    # Rewards
    reward_name: "???"
    reward_shaping: true
    reward_old_min: 0
    reward_old_max: 100
    reward_shaping_min: -10
    reward_shaping_max: 10
    # Warmup
    warmup_training_mode: null
    warmup_num_epochs: null
    num_warmup_batches_per_epoch: null
    # Hacks
    hack_truncate_length_constant: null
    # MLP-specific parameters
    mlp_policy_lm: "distilgpt2"
    mlp_input_specific: null
    mlp_logit_bias: 0
    mlp_fluent_prompt: false
    # prompt reward parameters
    prompt_task_lm: null
    prompt_dataset: null
    prompt_dataset_seed: null
    prompt_dataset_basepath: "."
    # text style transfer arguments
    tst_clf_basepath: null
    tst_n_repeats: 4
    tst_num_samples: 32 # Num of samples from which to take the output
    tst_num_bootstraps: 4 # Num of bootstraps to reduce reward randomness
    # classification arguments
    clf_kshot: 16
    clf_num_classes: -1
